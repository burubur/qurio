## Crawler Integration
-   **Engine:** Distributed Python Worker (Crawl4AI + Docling + Pynsq).
-   **Configuration:**
    -   `MaxDepth` (int): 0 = Single page, >0 = Recursive.
    -   `Exclusions` ([]string): Regex patterns to skip URLs.
-   **Pipeline:**
    -   **Producer (Go):** Publishes `{ type, url, id, depth, exclusions }` to `ingest.task` topic.
    -   **Worker (Python):**
        -   Consumes `ingest.task` using `pynsq.Reader`.
        -   **Web:** Uses `crawl4ai` (AsyncWebCrawler) to crawl and extract Markdown.
        -   **File:** Uses `docling` (DocumentConverter) to convert documents to Markdown.
        -   Publishes `{ source_id, content, url }` to `ingest.result` topic using `pynsq.Writer`.
    -   **Result Consumer (Go):**
        -   Consumes `ingest.result` using `go-nsq`.
        -   Computes BodyHash.
        -   Chunks content (512 tokens).
        -   Embeds chunks (Gemini).
        -   Stores chunks in Weaviate.
-   **Shared Storage:** `/tmp/qurio-uploads` mounted for file processing.