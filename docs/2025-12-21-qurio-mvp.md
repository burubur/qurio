# Product Requirements Document (PRD)

## Qurio - Open Source Context OS

**Document Metadata**

- **Version:** 5.1 (Configurable Reranking)
- **Date Created:** 2025-11-20
- **Last Updated:** 2025-12-20
- **Author(s):** Ichsan
- **Status:** Implementation Ready
- **Target Release:** v1.0.0 - Q1 2026
- **License:** Apache 2.0

***

## 1. EXECUTIVE SUMMARY

**Problem Statement:**
AI agents (Cursor, Windsurf, custom automation scripts) lack access to grounded, structured context from heterogeneous documentation sources (web pages, repositories, PDFs). Current solutions require cloud dependencies, authentication barriers, or proprietary APIs, creating friction for developers who need instant, local-first knowledge retrieval. Without a unified ingestion and retrieval system, developers experience context fragmentation, leading to hallucinated responses, incomplete code suggestions, and repeated manual context gathering that reduces productivity by an estimated 30-40% across daily coding workflows.

**Proposed Solution:**
Qurio is a self-hosted, open-source ingestion and retrieval engine that functions as a local Shared Library for AI agents. Built on PostgreSQL + Weaviate architecture with pure Go orchestration, it ingests heterogeneous documentation (web crawls, PDFs, DOCX, Markdown) and exposes standardized context retrieval via the Model Context Protocol (MCP). The system operates localhost-only without authentication barriers, supports configurable reranking pipelines (Jina AI, Cohere, or disabled) for precision tuning, and prioritizes llms.txt/llms-full.txt standards for documentation ingestion.

**Success Metrics:**

- **Time to First Query:** < 5 minutes from `docker-compose up` to successful MCP query response
- **Context Recall @ 5:** > 90% accuracy with reranking enabled (measured via RAGAS evaluation framework)
- **GitHub Adoption:** 100+ stars/forks within first month of public release
- **Query Latency (p95):** < 500ms for hybrid search without reranking; < 2s with reranking enabled
- **Ingestion Throughput:** Process 1000 documentation pages per hour per CPU core

**Stakeholders:**

- **Product Owner:** Ichsan (Author)
- **Technical Lead:** Ichsan (Author)
- **Key Dependencies:** Weaviate OSS Team (vector database), PostgreSQL Community (metadata storage), Model Context Protocol Working Group (protocol specification)

***

## 2. SCOPE DEFINITION

### 2.1 In Scope

- User can deploy entire system with single `docker-compose up` command on localhost
- User can upload documents (PDF, DOCX, HTML, MD, TXT) via web-based admin panel for ingestion
- System will crawl web URLs with recursive depth control and regex exclusion rules
- System will prioritize llms.txt and llms-full.txt files when detected at documentation root paths
- System will perform OCR on scanned documents and images via Docling processing pipeline
- System will generate high-dimensional vector embeddings using gemini-embedding-001 for all document chunks
- System will expose MCP JSON-RPC 2.0 endpoint over HTTP for AI agent queries
- System will support configurable reranking via Jina AI, Cohere, or disabled mode for retrieval precision tuning
- User can manually trigger re-sync operations for specific knowledge sources via admin panel
- System will provide real-time ingestion status updates (Processing → Embedding → Complete) without page refreshes
- System will perform SHA-256 hash deduplication on file uploads to prevent duplicate ingestion
- System will automatically retry failed ingestion jobs up to 3 times before routing to Dead Letter Queue
- System will log all queries (input/output) to stdout and file for debugging and evaluation
- Admin UI will comply with WCAG 2.1 AA accessibility standards


### 2.2 Out of Scope

- Multi-tenancy and user authentication (single-user localhost scope only)
- Source code ingestion with AST analysis (.java, .py, .ts files) - deferred to Growth Features
- Dynamic query routing using LLMs to select search strategy - deferred to Growth Features
- Metadata auto-tagging with LLM extraction for faceted search - deferred to Growth Features
- Advanced GraphRAG with cross-reference linking via FalkoreDB/Neo4j - deferred to Growth Features
- Temporal graph tracking context changes over time - deferred to Growth Features
- Mobile-responsive admin UI (desktop-only for MVP)
- Export/import of knowledge graph snapshots
- Real-time collaborative editing of documentation sources
- Integration with external notification systems (Slack, Discord, email)


### 2.3 Assumptions

- Target users have Docker and Docker Compose installed on their development machines
- Target users operate on Linux, macOS (Intel/Silicon), or Windows WSL2 environments
- PostgreSQL 15+ and Weaviate 1.24+ container images are accessible via Docker Hub
- Gemini API key is provided by user via environment variable for embedding generation
- Users have at least 8GB RAM available for container allocation (4GB Weaviate, 2GB PostgreSQL, 2GB Go services)
- Network access is available for initial documentation crawling and embedding API calls
- Documentation sources are primarily English-language (multilingual support deferred)
- Users understand basic Docker commands and can read service logs for troubleshooting


### 2.4 Dependencies

- **Weaviate OSS:** Vector database for hybrid search (BM25 + vector similarity). Requires v1.24+ for stable hybrid search API
- **PostgreSQL:** Relational database for metadata storage (sources, ingestion jobs, configurations). Requires v15+ for JSONB performance optimizations
- **Docling Library:** Document processing pipeline for OCR and layout analysis. Requires Python 3.10+ runtime environment
- **NSQ Message Queue:** Distributed task queue for async ingestion jobs. Requires nsqd and nsqlookupd services
- **Gemini Embedding API:** Google's gemini-embedding-001 model for vector generation. Requires valid API key with usage quota
- **Model Context Protocol Specification:** JSON-RPC 2.0 standard maintained by Anthropic and partners. Adherence to specification version 2025-03-26 or later
- **Reranking Providers (Optional):** Jina AI reranker v2 or Cohere rerank-english-v3.0 API access
- **Go 1.22+:** For building pure Go orchestration services
- **Vue 3 + TypeScript:** For admin panel frontend development

***

## 3. USER STORIES \& USE CASES

### 3.1 Primary User Personas

**Persona 1: AI-Assisted Developer**

- **Goals:**
    - Provide grounded context to AI coding assistants (Cursor, Windsurf) from private documentation
    - Eliminate hallucinated API suggestions by grounding models in actual documentation
    - Reduce context-switching between IDE and browser documentation tabs
- **Pain Points:**
    - AI models suggest outdated or incorrect API usage from generic training data
    - Manually copying documentation into IDE context windows is tedious and limited by token windows
    - Cloud-based RAG solutions expose proprietary documentation to third-party services
- **Technical Context:**
    - Proficient with Docker and command-line tools
    - Works primarily in Go, TypeScript, Python ecosystems
    - Uses NeoVim or VS Code with AI extensions
    - Requires localhost-only solutions for security/compliance

**Persona 2: Technical Documentation Curator**

- **Goals:**
    - Maintain a centralized, searchable knowledge base from scattered documentation sources
    - Ensure documentation freshness by re-syncing external sources on demand
    - Provide team members with consistent context retrieval across projects
- **Pain Points:**
    - Documentation scattered across Confluence, GitHub wikis, external vendor docs
    - No unified search across heterogeneous sources
    - Static documentation becomes stale without manual refresh processes
- **Technical Context:**
    - Comfortable with web-based admin interfaces
    - Manages documentation as part of DevOps/Platform Engineering role
    - Requires audit trails of what context was provided to AI agents

**Persona 3: Machine Learning Engineer (Retrieval Systems)**

- **Goals:**
    - Experiment with different retrieval strategies (BM25, vector, hybrid, reranked)
    - Evaluate retrieval quality using standard metrics (recall, precision, MRR)
    - Fine-tune embedding models or reranking pipelines for domain-specific docs
- **Pain Points:**
    - Closed-source RAG systems don't expose retrieval internals for tuning
    - Building retrieval pipelines from scratch is time-intensive
    - Lack of standardized evaluation frameworks for retrieval quality
- **Technical Context:**
    - Expert in Python, vector databases, and ML evaluation frameworks
    - Familiar with RAGAS, BEIR, and other IR benchmarks
    - Requires queryable logs and metrics for analysis


### 3.2 User Stories

**Story 1: Single-Command Deployment**

- **As a** AI-Assisted Developer
- **I want to** launch the entire Qurio system with a single `docker-compose up` command
- **So that** I can start querying documentation context within 5 minutes without complex configuration
- **Acceptance Criteria:**

1. GIVEN a fresh system with Docker installed WHEN I run `docker-compose up -d` THEN all services (PostgreSQL, Weaviate, Go API, Admin UI) start without errors within 60 seconds
2. GIVEN services are running WHEN I navigate to http://localhost:3000 THEN the admin panel loads successfully
3. GIVEN services are running WHEN I send a health check request to http://localhost:8080/health THEN I receive a 200 OK response with service status JSON

**Story 2: Web Documentation Crawling**

- **As a** Technical Documentation Curator
- **I want to** submit a documentation URL with crawl depth and exclusion rules
- **So that** the system ingests relevant pages while avoiding release notes and marketing content
- **Acceptance Criteria:**

1. GIVEN I am on the "Add Source" page WHEN I enter https://docs.example.com with depth=3 and exclusion regex `/release-notes|/blog` THEN the system validates the URL format and schedules a crawl job
2. GIVEN a crawl job is running WHEN I view the "Sources" list THEN I see real-time status updates (Crawling: 45/120 pages processed)
3. GIVEN a crawl completes WHEN I navigate to the source detail page THEN I see a list of all ingested URLs with success/failure indicators
4. GIVEN the site has llms.txt at root WHEN the crawler detects it THEN the system prioritizes ingestion of URLs listed in llms.txt before recursive crawl

**Story 3: Document Upload and OCR**

- **As a** AI-Assisted Developer
- **I want to** upload scanned PDF documents and have text automatically extracted via OCR
- **So that** legacy documentation in image-based PDFs becomes queryable context
- **Acceptance Criteria:**

1. GIVEN I am on the "Upload Document" page WHEN I drag-and-drop a scanned PDF THEN the system displays upload progress and initiates OCR processing via Docling
2. GIVEN a document is processing WHEN OCR completes THEN the extracted text is chunked and embedded without manual intervention
3. GIVEN I upload a duplicate file (same SHA-256) WHEN the system checks the hash THEN it displays "Duplicate detected: already ingested on [date]" and skips processing
4. GIVEN a document upload fails WHEN the error is transient (network timeout) THEN the system retries up to 3 times before moving to DLQ

**Story 4: MCP Query Interface**

- **As a** AI-Assisted Developer
- **I want to** query documentation context via MCP protocol from my IDE
- **So that** my AI coding assistant receives grounded context without leaving the editor
- **Acceptance Criteria:**

1. GIVEN Qurio is running WHEN my IDE sends a JSON-RPC 2.0 MCP query `{"jsonrpc":"2.0","method":"tools/call","params":{"name":"search","arguments":{"query":"authentication middleware"}}}` THEN the system returns relevant documentation chunks with source citations
2. GIVEN reranking is enabled WHEN I query "async error handling patterns" THEN results are reordered by relevance score and top 5 chunks have >0.8 reranker confidence
3. GIVEN a query returns no high-confidence results WHEN all scores are <0.3 threshold THEN the system returns an empty result set rather than low-quality matches (prioritize precision over recall)

**Story 5: Source Re-sync**

- **As a** Technical Documentation Curator
- **I want to** manually trigger a re-sync of a specific documentation source
- **So that** I can update context after upstream documentation changes without full system re-ingestion
- **Acceptance Criteria:**

1. GIVEN I am viewing a source detail page WHEN I click "Re-sync Now" THEN the system schedules a new crawl/upload job for that source only
2. GIVEN a re-sync job runs WHEN new pages are detected THEN they are added to the index; when pages are removed upstream THEN they are marked inactive (soft delete)
3. GIVEN a re-sync completes WHEN I query content from that source THEN results reflect the latest version of the documentation

**Story 6: Configurable Reranking**

- **As a** Machine Learning Engineer
- **I want to** enable/disable reranking and select a provider (Jina AI, Cohere, None)
- **So that** I can compare retrieval quality across different precision strategies
- **Acceptance Criteria:**

1. GIVEN I am in Settings WHEN I select "Reranker: Jina AI v2" and save THEN all subsequent queries use Jina AI reranking API
2. GIVEN reranking is disabled WHEN I query documentation THEN results are returned from Weaviate hybrid search without reranking latency overhead
3. GIVEN reranking is enabled WHEN I view query logs THEN I see both pre-rerank and post-rerank result orderings with score deltas


### 3.3 Use Cases

**Use Case 1: Initial System Setup**

- **Actor:** AI-Assisted Developer
- **Preconditions:** Docker and Docker Compose installed; no previous Qurio installation
- **Main Flow:**

1. Developer clones Qurio repository from GitHub
2. Developer copies `.env.example` to `.env` and sets `GEMINI_API_KEY=<key>`
3. Developer runs `docker-compose up -d` from project root
4. System pulls container images (PostgreSQL, Weaviate, Go services)
5. System runs database migrations and initializes Weaviate schema
6. System starts API server on port 8080 and admin UI on port 3000
7. Developer navigates to http://localhost:3000 and sees empty dashboard
8. Developer adds first documentation source (URL or file upload)
- **Postconditions:** Qurio is running and ready to ingest documentation
- **Alternative Flows:**
    - **3a.** Port 8080 already in use: System logs error and exits; developer changes `API_PORT` in `.env` and restarts
    - **5a.** Weaviate fails to initialize schema: System retries 3 times with exponential backoff; if all retries fail, logs detailed error and exits

**Use Case 2: Web Documentation Ingestion with Exclusions**

- **Actor:** Technical Documentation Curator
- **Preconditions:** Qurio system is running; user is on admin panel
- **Main Flow:**

1. User clicks "Add Source" button in admin panel
2. System displays source creation form
3. User selects "Web Crawl" source type
4. User enters URL: `https://docs.encore.dev`
5. User sets crawl depth: 3 (pages reachable within 3 links)
6. User enables "Detect llms.txt" checkbox
7. User adds exclusion regex: `/release-notes|/changelog|/pricing`
8. User clicks "Start Crawl"
9. System validates URL is reachable (HTTP HEAD request)
10. System checks for llms.txt at `https://docs.encore.dev/llms.txt`
11. System finds llms.txt and parses markdown links
12. System prioritizes URLs from llms.txt, then performs recursive crawl respecting depth and exclusions
13. System displays real-time progress: "Crawling: 67/150 pages (llms.txt: 12/12 complete)"
14. System chunks each page into 512-token segments with 50-token overlap
15. System generates embeddings via Gemini API for all chunks
16. System indexes chunks in Weaviate with metadata (source_id, url, title, crawl_date)
17. System marks source status as "Complete" and displays ingestion statistics
- **Postconditions:** Documentation is queryable via MCP; source is listed in admin panel with metadata
- **Alternative Flows:**
    - **9a.** URL is unreachable (DNS failure, 404): System displays error "Could not reach URL: [error details]"; user corrects URL and resubmits
    - **10a.** No llms.txt found: System proceeds with standard recursive crawl from base URL
    - **12a.** Crawler hits rate limit (429 response): System implements exponential backoff (1s, 2s, 4s, 8s); logs rate limit event
    - **15a.** Gemini API quota exceeded: System pauses ingestion, displays error "Embedding quota exceeded. Resume after quota reset or change API key"; user can retry after quota refresh

**Use Case 3: Querying Documentation via MCP from IDE**

- **Actor:** AI-Assisted Developer (via Cursor IDE)
- **Preconditions:** Qurio is running; documentation has been ingested; IDE is configured with MCP endpoint http://localhost:8080/mcp
- **Main Flow:**

1. Developer types question in Cursor chat: "How do I configure PostgreSQL connection pooling in Encore?"
2. Cursor IDE constructs MCP JSON-RPC request:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "method": "tools/call",
  "params": {
    "name": "search",
    "arguments": {
      "query": "PostgreSQL connection pooling configuration Encore"
    }
  }
}
```

3. IDE sends HTTP POST to http://localhost:8080/mcp
4. Qurio API parses JSON-RPC request and extracts search query
5. System performs Weaviate hybrid search (BM25 + vector similarity) with query
6. System retrieves top 20 candidate chunks from Weaviate
7. System checks reranker configuration (enabled: Jina AI)
8. System sends candidate chunks to Jina AI reranker API with query
9. Reranker returns reordered chunks with relevance scores (0.0-1.0)
10. System filters chunks below 0.3 confidence threshold
11. System constructs MCP response with top 5 reranked chunks:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "result": {
    "content": [
      {
        "type": "text",
        "text": "Database Configuration\n\nEncore automatically manages connection pooling...",
        "metadata": {
          "source": "docs.encore.dev/database/config",
          "score": 0.94
        }
      }
    ]
  }
}
```

12. System logs query and response to stdout and file
13. IDE receives response and displays context to developer
14. Developer uses grounded context to write accurate code
- **Postconditions:** Query is logged; developer has accurate context
- **Alternative Flows:**
    - **7a.** Reranker is disabled: System returns top 5 chunks from Weaviate hybrid search without reranking (skips steps 8-10)
    - **8a.** Reranker API fails (network error): System falls back to Weaviate results; logs warning "Reranker failed, using hybrid search fallback"
    - **10a.** All chunks below 0.3 threshold: System returns empty result set with message "No high-confidence results found"; prevents hallucination from low-quality matches

**Use Case 4: Re-syncing Stale Documentation**

- **Actor:** Technical Documentation Curator
- **Preconditions:** Documentation source "Encore Docs" was ingested 30 days ago; upstream docs have been updated
- **Main Flow:**

1. User navigates to "Sources" page in admin panel
2. User clicks on "Encore Docs" source row
3. System displays source detail page with metadata (last sync date, page count, status)
4. User clicks "Re-sync Now" button
5. System confirms action with modal: "Re-sync will update all pages from this source. Continue?"
6. User clicks "Confirm"
7. System schedules re-sync job in NSQ task queue
8. System re-crawls documentation using original configuration (depth, exclusions)
9. System compares SHA-256 hashes of page content with stored hashes
10. System identifies 15 changed pages, 3 new pages, 2 deleted pages
11. System re-chunks and re-embeds changed and new pages
12. System marks deleted pages as inactive (soft delete) with deletion timestamp
13. System updates source metadata (last_sync: current timestamp, page_count: new total)
14. System displays completion notification: "Re-sync complete: 15 updated, 3 added, 2 removed"
- **Postconditions:** Documentation reflects latest upstream changes; queries return updated content
- **Alternative Flows:**
    - **8a.** Source URL is unreachable: System displays error "Source unreachable. Upstream documentation may have moved or been deleted"; user can update URL or delete source
    - **11a.** Re-embedding fails due to API quota: System pauses at current progress; displays "Re-sync paused: [X] of [Y] pages updated. Resume after quota refresh"

***

## 4. FUNCTIONAL REQUIREMENTS

### 4.1 Deployment \& Core System

**FR-1.1: Single Command Deployment**

- **Description:** The system SHALL launch all required services (PostgreSQL, Weaviate, Go API, Admin UI) via a single `docker-compose up` command on localhost.
- **Input:**
    - Command: `docker-compose up -d`
    - Environment variables in `.env` file: `GEMINI_API_KEY`, `API_PORT`, `UI_PORT`, `POSTGRES_PASSWORD`, `WEAVIATE_VOLUME`
- **Processing:**
    - Docker Compose orchestrates service startup in dependency order (PostgreSQL → Weaviate → API → UI)
    - Go API service runs database migrations on startup
    - API service initializes Weaviate schema (classes, properties, indexes)
    - Health checks confirm all services are responsive before marking system ready
- **Output:**
    - All services running and accessible on localhost
    - Admin UI available at http://localhost:3000
    - MCP API available at http://localhost:8080/mcp
    - Health endpoint returns 200 OK at http://localhost:8080/health
- **Business Rules:**
    - Startup SHALL complete within 60 seconds on systems meeting minimum requirements (8GB RAM, 4 CPU cores)
    - If GEMINI_API_KEY is not set, system SHALL log warning and continue (ingestion will fail without embeddings, but other functions operational)
    - PostgreSQL and Weaviate data SHALL persist in Docker volumes across restarts
- **Acceptance Criteria:**

1. GIVEN a fresh system with Docker 24.0+ installed
WHEN developer runs `docker-compose up -d`
THEN all containers start and health checks pass within 60 seconds
2. GIVEN services are running
WHEN developer navigates to http://localhost:3000
THEN admin panel loads with empty dashboard (no sources, no queries)
3. GIVEN services are running
WHEN developer sends GET http://localhost:8080/health
THEN response is `{"status":"healthy","services":{"postgres":"up","weaviate":"up","api":"up"},"version":"1.0.0"}`

**FR-1.2: Global Namespace Scope**

- **Description:** The system SHALL operate in a single global namespace where all ingested documentation is accessible without authentication, authorization, or tenant isolation.
- **Input:** None (architectural constraint)
- **Processing:**
    - All Weaviate classes use default namespace
    - PostgreSQL tables use single schema without tenant_id columns
    - API endpoints do not require JWT, OAuth, or API key validation
- **Output:** All queries return results from entire knowledge base without access control filtering
- **Business Rules:**
    - System is designed for single-user localhost deployment only
    - Network access SHALL be restricted to localhost (127.0.0.1) via Docker network configuration
    - Multi-tenancy is explicitly out of scope for MVP
- **Acceptance Criteria:**

1. GIVEN documentation from multiple sources is ingested
WHEN a user queries via MCP
THEN results include chunks from all sources without access control filtering
2. GIVEN no authentication token is provided
WHEN a request is sent to any API endpoint
THEN request is processed successfully (no 401/403 responses)

**FR-1.3: Localhost-Only Access**

- **Description:** The system SHALL expose all services (admin UI, API, MCP endpoint) exclusively on localhost without external network access or authentication requirements.
- **Input:** Docker Compose network configuration
- **Processing:**
    - All services bind to 127.0.0.1 interface only
    - Docker network is configured as internal (no external routing)
    - No authentication middleware in API request pipeline
- **Output:** Services accessible only from localhost; external requests fail
- **Business Rules:**
    - Binding to 0.0.0.0 SHALL be disabled by default (security best practice)
    - Users can override for LAN access via environment variable `ALLOW_LAN_ACCESS=true` (not recommended)
- **Acceptance Criteria:**

1. GIVEN Qurio is running
WHEN a request is sent from external machine to http://<docker-host-ip>:8080
THEN connection is refused (no route to host)
2. GIVEN Qurio is running
WHEN developer sends request from localhost to http://localhost:8080/health
THEN request succeeds with 200 OK


### 4.2 Document Ingestion Pipeline

**FR-2.1: Document Upload Interface**

- **Description:** The system SHALL provide a web-based file upload interface supporting PDF, DOCX, HTML, MD, and TXT formats via drag-and-drop or file picker.
- **Input:**
    - User-selected files via browser file input or drag-and-drop
    - Supported MIME types: `application/pdf`, `application/vnd.openxmlformats-officedocument.wordprocessingml.document`, `text/html`, `text/markdown`, `text/plain`
    - Maximum file size: 50MB per file
- **Processing:**
    - Frontend validates file type and size before upload
    - File is uploaded to API endpoint `/api/sources/upload` via multipart/form-data POST
    - Backend calculates SHA-256 hash of file content
    - Backend checks hash against existing documents in PostgreSQL
    - If duplicate, ingestion is skipped; if unique, file is saved to temporary storage
    - NSQ job is enqueued with file path and source metadata
- **Output:**
    - Upload progress indicator (0-100%)
    - Success message: "File uploaded successfully. Ingestion started."
    - Duplicate message: "Duplicate detected: This file was already ingested on [date]"
    - Error message: "Upload failed: [specific error]"
- **Business Rules:**
    - Files exceeding 50MB SHALL be rejected with clear error message
    - Unsupported file types SHALL be rejected with list of supported formats
    - Duplicate detection SHALL use SHA-256 to compare binary content (not filename)
- **Acceptance Criteria:**

1. GIVEN user is on "Upload Document" page
WHEN user drags a 10MB PDF file into drop zone
THEN file uploads with progress bar and "Ingestion started" message appears
2. GIVEN user uploads file "manual.pdf" with hash abc123
WHEN user uploads same file again (same hash)
THEN system displays "Duplicate detected: already ingested on 2025-12-15" without re-processing
3. GIVEN user attempts to upload 60MB PDF
WHEN upload is initiated
THEN frontend rejects with error "File exceeds 50MB limit" before upload starts

**FR-2.2: OCR Processing via Docling**

- **Description:** The system SHALL automatically extract text from images and scanned documents using Docling OCR pipeline when digital text is unavailable.
- **Input:**
    - Uploaded file (PDF, image formats)
    - Docling `PdfFormatOption` configuration with `do_ocr=true` flag
- **Processing:**
    - Docling analyzes document layout and detects text layers
    - If embedded text exists (digital PDF), Docling extracts text directly (skips OCR)
    - If text layer is missing (scanned PDF/image), Docling invokes vision model for OCR
    - Extracted text is structured with layout information (paragraphs, tables, lists)
    - Text is normalized (whitespace cleanup, encoding validation)
- **Output:**
    - Plain text content extracted from document
    - Metadata: OCR confidence scores, layout regions detected
    - Processing time logged for performance monitoring
- **Business Rules:**
    - OCR SHALL be skipped for documents with extractable digital text (performance optimization)
    - Low OCR confidence regions (<0.5 confidence) SHALL be flagged in metadata for manual review
    - OCR processing SHALL timeout after 5 minutes per document to prevent hanging jobs
- **Acceptance Criteria:**

1. GIVEN a scanned PDF with no text layer
WHEN document is processed by Docling
THEN text is extracted via OCR and available for chunking
1. GIVEN a digital PDF with embedded text
WHEN document is processed by Docling
THEN text is extracted directly without OCR (faster processing)
1. GIVEN a document with mixed content (digital text + scanned images)
WHEN document is processed
THEN digital text is extracted directly and OCR is applied only to image regions

**FR-2.3: SHA-256 Deduplication**

- **Description:** The system SHALL prevent duplicate ingestion of identical documents by comparing SHA-256 hashes before processing.
- **Input:**
    - File binary content
    - Existing document hashes in PostgreSQL `documents` table
- **Processing:**
    - Calculate SHA-256 hash of uploaded file content using Go `crypto/sha256` package
    - Query PostgreSQL: `SELECT id, ingested_at FROM documents WHERE sha256_hash = $1`
    - If match found, return existing document metadata without re-ingestion
    - If no match, proceed with ingestion pipeline
- **Output:**
    - Duplicate detected: Return existing document ID and ingestion date
    - Unique document: Proceed to ingestion and return new document ID
- **Business Rules:**
    - Hash comparison SHALL use SHA-256 (not MD5 or filename comparison)
    - Renamed files with identical content SHALL be detected as duplicates
    - Users SHALL be able to force re-ingestion via "Override Duplicate Check" option (updates existing document)
- **Acceptance Criteria:**

1. GIVEN document with hash abc123 exists in system
WHEN user uploads file with same hash
THEN system returns "Duplicate: ingested on 2025-12-01" without processing
2. GIVEN document "guide_v1.pdf" is ingested
WHEN user uploads "guide_v2_renamed.pdf" with identical binary content
THEN system detects duplicate despite filename difference
3. GIVEN user enables "Override Duplicate Check"
WHEN duplicate is uploaded
THEN system re-processes document and updates existing record with new chunks

**FR-2.4: Automatic Retry with Dead Letter Queue**

- **Description:** The system SHALL automatically retry failed ingestion jobs up to 3 times with exponential backoff, then route persistent failures to a Dead Letter Queue for manual inspection.
- **Input:**
    - NSQ message for ingestion job
    - Failure error (network timeout, API error, parsing failure)
    - Retry count metadata
- **Processing:**
    - NSQ consumer attempts job processing
    - On failure, NSQ automatically requeues message with incremented attempt counter
    - Exponential backoff: 1st retry after 10s, 2nd after 60s, 3rd after 300s
    - After 3rd failure, message is published to DLQ topic `ingestion_dlq`
    - DLQ messages are stored in PostgreSQL `failed_jobs` table with full error context
- **Output:**
    - Successful retry: Job completes and source status updates
    - Failed to DLQ: Admin panel displays failed job with error details and "Retry" button
- **Business Rules:**
    - Transient errors (network timeouts, 429 rate limits) SHALL be retried automatically
    - Permanent errors (invalid file format, 404 URLs) SHALL fail immediately without retries
    - DLQ messages SHALL be retained for 30 days before automatic deletion
- **Acceptance Criteria:**

1. GIVEN ingestion job fails with "Gemini API timeout"
WHEN NSQ processes retry logic
THEN job is retried 3 times with backoff (10s, 60s, 300s) before moving to DLQ
1. GIVEN ingestion job fails with "Invalid PDF format"
WHEN error is classified as permanent
THEN job moves directly to DLQ without retries (logs "Permanent error detected")
1. GIVEN job is in DLQ
WHEN admin views failed jobs page
THEN full error stacktrace and input parameters are displayed with "Retry" button

**FR-2.5: Contextual Embeddings via Gemini**

- **Description:** The system SHALL generate high-dimensional vector embeddings for all document chunks using Google's gemini-embedding-001 model to enable semantic search.
- **Input:**
    - Text chunk (512 tokens with 50-token overlap)
    - Gemini API key from environment variable
    - Model specification: `models/embedding-001` (768-dimensional output)
- **Processing:**
    - Chunk text is sent to Gemini API via HTTPS POST
    - API returns embedding vector (float32 array, 768 dimensions)
    - Vector is normalized to unit length (L2 normalization)
    - Embedding is stored in Weaviate with chunk text and metadata
- **Output:**
    - 768-dimensional float32 vector stored in Weaviate `DocumentChunk` class
    - Embedding generation latency logged (target: <200ms p95)
- **Business Rules:**
    - Embedding generation SHALL be batched (up to 100 chunks per API call) for efficiency
    - API rate limits (60 requests/minute) SHALL be respected with client-side throttling
    - Failed embeddings SHALL be retried per FR-2.4 retry logic
- **Acceptance Criteria:**

1. GIVEN a document is chunked into 50 segments
WHEN embeddings are generated
THEN 50 vectors are stored in Weaviate with corresponding chunk_id references
2. GIVEN Gemini API returns 429 rate limit error
WHEN embedding job processes response
THEN job pauses for 60 seconds before retrying (respects rate limit)
3. GIVEN embedding vector is returned
WHEN vector is stored in Weaviate
THEN vector magnitude is 1.0 (normalized) and dimensionality is 768


### 4.3 Web Ingestion Pipeline

**FR-3.1: Web Crawl Management with Exclusions**

- **Description:** The system SHALL accept web URLs for crawling with configurable recursive depth and regex-based exclusion rules to filter irrelevant paths.
- **Input:**
    - Base URL (e.g., `https://docs.encore.dev`)
    - Crawl depth: integer 0-5 (0 = single page, 5 = max depth)
    - Exclusion regex list (e.g., `/release-notes`, `/pricing`, `/blog`)
    - Respect robots.txt: boolean (default: true)
- **Processing:**
    - URL is validated via HTTP HEAD request (check 200 response)
    - Crawler starts from base URL and follows links respecting depth limit
    - For each discovered URL, regex exclusions are applied
    - Matched exclusions are logged and skipped
    - Crawled pages are parsed to extract content and outbound links
    - Process repeats until depth limit reached or no new links found
- **Output:**
    - List of crawled URLs with status (success, excluded, failed)
    - Real-time progress indicator: "Crawling: 45/120 pages (15 excluded)"
    - Crawl summary: total pages, excluded count, error count, duration
- **Business Rules:**
    - Depth 0 SHALL crawl only the specified URL without following links
    - Regex exclusions SHALL be applied to full URL path (not just domain)
    - Crawl rate SHALL be limited to 5 requests/second per domain (politeness policy)
    - URLs returning 404 SHALL be logged but not marked as errors (expected for link rot)
- **Acceptance Criteria:**

1. GIVEN URL `https://docs.example.com` with depth=2 and exclusion `/changelog|/blog`
WHEN crawl executes
THEN pages under `/changelog` and `/blog` are skipped and logged as excluded
2. GIVEN base URL has 100 pages within depth 3
WHEN crawl runs with depth=2
THEN maximum 50 pages are crawled (pages at depth 3 are not reached)
3. GIVEN crawl encounters URL with query parameters `?page=2`
WHEN crawler normalizes URLs
THEN duplicate URLs with different query params are deduplicated based on canonical URL

**FR-3.2: Sitemap.xml Support**

- **Description:** The system SHALL detect and parse sitemap.xml files to define crawl scope precisely, prioritizing sitemap URLs over recursive discovery.
- **Input:**
    - Base URL (e.g., `https://docs.example.com`)
    - Sitemap URL (auto-detected at `/sitemap.xml` or manually specified)
- **Processing:**
    - System sends GET request to `/sitemap.xml` at base URL
    - If 200 OK, parse XML structure per sitemaps.org specification

```
- Extract all `<loc>` URLs and optional `<lastmod>` timestamps
```

    - If sitemap index found (links to multiple sitemaps), recursively fetch all sub-sitemaps
    - URLs from sitemap are queued for crawling before recursive link following
    - `<lastmod>` timestamps are compared with stored metadata to prioritize changed pages
- **Output:**
    - List of URLs from sitemap with lastmod dates
    - Crawl plan: sitemap URLs processed first, then recursive crawl if enabled
- **Business Rules:**
    - If sitemap is found, sitemap URLs SHALL take priority over recursive crawl
    - `<priority>` hints in sitemap MAY be used to order crawl queue (high priority first)
    - Sitemap SHALL be re-fetched on re-sync to detect new/updated pages
- **Acceptance Criteria:**

1. GIVEN base URL has sitemap at `/sitemap.xml` with 200 URLs
WHEN crawler detects sitemap
THEN all 200 URLs are queued for crawl before recursive link discovery starts
```
2. GIVEN sitemap contains `<lastmod>2025-12-01</lastmod>` for page A
```

WHEN system checks existing metadata (last crawled 2025-11-01)
THEN page A is prioritized for re-crawl (detected as updated)
3. GIVEN sitemap is unreachable (404)
WHEN crawler checks for sitemap
THEN crawler falls back to recursive crawl from base URL without error

**FR-3.3: JavaScript Rendering for SPAs**

- **Description:** The system SHALL perform JavaScript rendering via headless browser to access dynamically loaded content on Single Page Applications and modern documentation sites.
- **Input:**
    - Target URL
    - Wait condition: DOMContentLoaded, networkidle0, or custom selector
    - Timeout: 30 seconds default
- **Processing:**
    - Crawler spawns headless Chrome instance via Playwright or Puppeteer
    - Browser navigates to URL and waits for specified condition
    - JavaScript executes and DOM is rendered
    - Rendered HTML is extracted from final DOM state
    - Browser instance is closed to free resources
- **Output:**
    - Fully rendered HTML content including JS-generated elements
    - Screenshot (optional) for debugging failed renders
- **Business Rules:**
    - Static sites (no JS required) SHOULD skip headless rendering for performance (detect via Content-Type header)
    - Headless rendering SHALL timeout after 30 seconds to prevent hanging
    - Browser instances SHALL be pooled (max 5 concurrent) to limit resource usage
- **Acceptance Criteria:**

1. GIVEN URL `https://react-docs.example.com` (SPA with client-side rendering)
WHEN crawler fetches page
THEN headless browser waits for DOMContentLoaded and extracts rendered HTML (not empty React root div)
2. GIVEN page has delayed content loading (AJAX call after 2s)
WHEN crawler uses `networkidle0` wait condition
THEN crawler waits until network is idle before extracting content
3. GIVEN headless render exceeds 30s timeout
WHEN timeout is reached
THEN browser is killed, partial content is extracted, and warning is logged

**FR-3.4: llms.txt Priority Ingestion**

- **Description:** The system SHALL prioritize ingestion of llms.txt and llms-full.txt files when detected at documentation root, treating them as authoritative context sources per the specification.
- **Input:**
    - Base URL (e.g., `https://docs.encore.dev`)
    - llms.txt URL: `{base_url}/llms.txt`
    - llms-full.txt URL: `{base_url}/llms-full.txt`
- **Processing:**
    - Before starting recursive crawl, system checks for `{base_url}/llms.txt`
    - If found, parse markdown structure to extract navigation links
    - System prioritizes crawling URLs listed in llms.txt
    - If `llms-full.txt` exists, check file size (<10MB recommended for context window limits)
    - If llms-full.txt is suitable size, ingest as single comprehensive document
    - If llms-full.txt is too large, fallback to llms.txt navigation approach
- **Output:**
    - llms.txt URLs crawled first (before recursive discovery)
    - llms-full.txt ingested as single cohesive document (if size appropriate)
    - Status indicator: "Processed llms.txt: 12/12 priority URLs complete"
- **Business Rules:**
    - llms.txt SHALL be treated as canonical navigation for AI-optimized docs
    - llms-full.txt SHALL be preferred if file size < 10MB (fits in context windows)
    - Standard recursive crawl SHALL still execute after llms.txt URLs (to catch unlisted pages)
- **Acceptance Criteria:**

1. GIVEN documentation site has llms.txt with 20 links
WHEN crawl starts
THEN those 20 URLs are crawled first before recursive link following begins
1. GIVEN site has llms-full.txt (5MB size)
WHEN crawler detects it
THEN file is downloaded and ingested as single document (not chunked by URL)
1. GIVEN llms-full.txt is 50MB (too large)
WHEN crawler checks file size
THEN crawler logs warning and falls back to llms.txt navigation approach


### 4.4 Context Management

**FR-4.1: Live Source Re-sync**

- **Description:** The system SHALL allow users to manually trigger re-processing of specific knowledge sources to update embeddings and indexes with latest content.
- **Input:**
    - Source ID (from admin panel)
    - Re-sync trigger: button click in UI
- **Processing:**
    - System retrieves original source configuration (URL, crawl settings, file path)
    - New ingestion job is scheduled in NSQ with `resync=true` flag
    - System re-crawls web sources or re-uploads files using original parameters
    - Content hashes (SHA-256) are compared with existing records
    - Changed content: existing chunks are deleted, new chunks are created and embedded
    - Unchanged content: embeddings are preserved (no re-processing)
    - New pages: chunks created and embedded
    - Deleted pages: marked inactive with `deleted_at` timestamp (soft delete)
- **Output:**
    - Re-sync progress indicator: "Re-syncing: 15/50 pages processed"
    - Completion summary: "15 updated, 3 added, 2 removed"
    - Source metadata updated: `last_synced_at` timestamp
- **Business Rules:**
    - Re-sync SHALL preserve source_id (updates existing source, not create new)
    - Soft-deleted chunks SHALL remain queryable for 7 days (grace period) then hard-deleted
    - Re-sync SHALL respect original exclusion rules and depth limits
- **Acceptance Criteria:**

1. GIVEN source "Encore Docs" was synced 30 days ago
WHEN user clicks "Re-sync Now"
THEN system re-crawls documentation and updates changed pages only (preserves unchanged embeddings)
2. GIVEN upstream documentation removed page `/old-guide`
WHEN re-sync completes
THEN chunks from `/old-guide` are marked inactive and excluded from queries after 7-day grace period
3. GIVEN re-sync detects 10 unchanged pages and 5 updated pages
WHEN re-sync completes
THEN only 5 pages are re-embedded (10 unchanged pages skip embedding for efficiency)

**FR-4.2: Source Management CRUD**

- **Description:** The system SHALL provide Create, Read, Update, Delete operations for knowledge sources via admin panel.
- **Input:**
    - Create: source name, type (web/file), configuration (URL, depth, exclusions)
    - Read: source_id
    - Update: source_id, updated fields (name, exclusions, enabled status)
    - Delete: source_id
- **Processing:**
    - Create: Insert record into PostgreSQL `sources` table, schedule initial ingestion
    - Read: Query source metadata and aggregate statistics (page count, last sync, status)
    - Update: Update source record, optionally trigger re-sync if configuration changed
    - Delete: Soft-delete source and all associated chunks (mark `deleted_at` timestamp)
- **Output:**
    - Create: New source ID and ingestion status
    - Read: Source detail view with metadata and statistics
    - Update: Success confirmation and updated metadata
    - Delete: Confirmation message and cleanup summary
- **Business Rules:**
    - Delete SHALL be soft-delete (data retained for 30 days before purge)
    - Updating exclusion rules SHOULD trigger re-sync prompt (warn user of needed re-process)
    - Source name MUST be unique within system
- **Acceptance Criteria:**

1. GIVEN user fills "Add Source" form with name "FastAPI Docs" and URL
WHEN form is submitted
THEN new source is created and initial crawl starts automatically
2. GIVEN source exists with ID 123
WHEN user updates exclusion regex from `/blog` to `/blog|/news`
THEN system updates configuration and displays "Re-sync recommended to apply new exclusions"
3. GIVEN user deletes source "Old API Docs"
WHEN delete is confirmed
THEN source and chunks are soft-deleted (retained 30 days) and excluded from queries immediately


### 4.5 Retrieval Pipeline

**FR-5.1: MCP JSON-RPC 2.0 Endpoint**

- **Description:** The system SHALL expose an MCP-compliant JSON-RPC 2.0 endpoint over HTTP for AI agent queries, adhering to the Model Context Protocol specification.
- **Input:**
    - HTTP POST to `/mcp`
    - Content-Type: `application/json`
    - Body:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "method": "tools/call",
  "params": {
    "name": "search",
    "arguments": {
      "query": "authentication middleware setup"
    }
  }
}
```

- **Processing:**
    - Validate JSON-RPC 2.0 structure (must have jsonrpc, id, method)
    - Extract search query from params.arguments
    - Execute retrieval pipeline (FR-5.2, FR-5.4)
    - Construct MCP response with results
- **Output:**
    - Success response:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "result": {
    "content": [
      {
        "type": "text",
        "text": "Authentication middleware can be configured...",
        "metadata": {
          "source": "docs.example.com/auth",
          "score": 0.92
        }
      }
    ]
  }
}
```

    - Error response:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "error": {
    "code": -32602,
    "message": "Invalid params: query is required"
  }
}
```

- **Business Rules:**
    - Endpoint SHALL accept only JSON-RPC 2.0 messages (reject 1.0 or missing version)
    - Request ID SHALL be echoed in response per spec
    - Errors SHALL use standard JSON-RPC error codes (-32700 parse error, -32600 invalid request, -32601 method not found, -32602 invalid params)
- **Acceptance Criteria:**

1. GIVEN valid MCP query with jsonrpc=2.0
WHEN request is sent to `/mcp`
THEN response includes same ID and result object with documentation chunks
1. GIVEN request missing jsonrpc field
WHEN request is sent
THEN response is error code -32600 "Invalid Request"
1. GIVEN request with unknown method "invalid/method"
WHEN request is sent
THEN response is error code -32601 "Method not found"

**FR-5.2: Weaviate Unified Hybrid Search**

- **Description:** The system SHALL use Weaviate Hybrid Search (BM25 + vector similarity) as the baseline retrieval strategy to leverage both keyword relevance and semantic similarity.
- **Input:**
    - Query string (e.g., "PostgreSQL connection pooling")
    - Top K: number of results to return (default: 20)
    - Alpha: hybrid search balance (0.0 = pure BM25, 1.0 = pure vector, 0.5 = balanced)
- **Processing:**
    - Query string is embedded via gemini-embedding-001 to get query vector
    - Weaviate executes parallel searches:
        - BM25 keyword search on `text` property
        - Vector similarity search on `embedding` property
    - Scores from both searches are normalized to 0-1 range
    - Final score = alpha × vector_score + (1-alpha) × bm25_score
    - Results are ranked by combined score
- **Output:**
    - Ranked list of chunks with combined scores
    - Metadata: chunk_id, source_url, text, bm25_score, vector_score, combined_score
- **Business Rules:**
    - Default alpha SHALL be 0.5 (balanced hybrid)
    - Users MAY override alpha in configuration for domain-specific tuning
    - Minimum score threshold SHALL be 0.1 (filter low-relevance results)
- **Acceptance Criteria:**

1. GIVEN query "database connection pool"
WHEN hybrid search executes with alpha=0.5
THEN results include high BM25 matches (exact keyword hits) and high vector matches (semantic similarity) combined
1. GIVEN query with typo "databse conection pool"
WHEN hybrid search executes
THEN vector search compensates for typo (semantic understanding) while BM25 score is low
1. GIVEN all results have combined score < 0.1
WHEN minimum threshold filter is applied
THEN empty result set is returned (prevents low-quality results)

**FR-5.3: Query Logging**

- **Description:** The system SHALL log all queries (input and output) to stdout and persistent file storage for debugging and evaluation.
- **Input:**
    - Query: user query string
    - Results: returned chunks with scores
    - Metadata: timestamp, latency, reranker status
- **Processing:**
    - On each query, construct log entry:

```json
{
  "timestamp": "2025-12-20T07:30:00Z",
  "query": "authentication middleware",
  "num_results": 5,
  "latency_ms": 234,
  "reranker": "jinaai",
  "top_score": 0.94,
  "results": [
    {
      "chunk_id": "abc123",
      "source": "docs.example.com/auth",
      "score": 0.94,
      "text_preview": "Authentication middleware can be..."
    }
  ]
}
```

    - Write to stdout (for Docker logs)
    - Append to `/data/logs/query.log` (persistent volume)
- **Output:**
    - Structured JSON log entries
    - Log file rotated daily (max 7 days retention)
- **Business Rules:**
    - Logs SHALL include full query and result metadata for reproducibility
    - PII in logs SHALL be redacted if detected (email addresses, API keys)
    - Logs MAY be exported to JSONL for offline evaluation with RAGAS
- **Acceptance Criteria:**

1. GIVEN query is executed
WHEN log is written
THEN stdout shows structured JSON with query, results, latency
2. GIVEN log file exceeds 100MB
WHEN log rotation triggers
THEN current file is renamed with timestamp and new log file started
3. GIVEN query contains email "user@example.com"
WHEN log is written
THEN email is redacted to "user@[REDACTED]" (PII protection)

**FR-5.4: Configurable Reranking Pipeline**

- **Description:** The system SHALL support optional reranking of search results via configurable providers (Jina AI, Cohere, or disabled) to refine result precision.
- **Input:**
    - Configuration: reranker provider (jinaai, cohere, none)
    - API key for selected provider (from environment variables)
    - Candidate chunks from hybrid search (top 20)
    - Query string
- **Processing:**
    - If reranker=none, return hybrid search results directly (skip reranking)
    - If reranker=jinaai:
        - Send query + candidate chunks to Jina AI reranker API
        - Model: `jina-reranker-v2-base-multilingual`
        - Receive reranked results with relevance scores (0.0-1.0)
    - If reranker=cohere:
        - Send query + candidate chunks to Cohere rerank API
        - Model: `rerank-english-v3.0`
        - Receive reranked results with relevance scores
    - Filter results below confidence threshold (0.3 default)
    - Return top 5 reranked chunks
- **Output:**
    - Reranked chunks ordered by provider relevance score
    - Metadata: pre-rerank score, post-rerank score, provider used
- **Business Rules:**
    - If reranker API fails, system SHALL fallback to hybrid search results (graceful degradation)
    - Reranker SHALL timeout after 3 seconds to maintain query latency SLA
    - Reranking cost (API calls) SHALL be logged for usage monitoring
- **Acceptance Criteria:**

1. GIVEN configuration `RERANKER=jinaai`
WHEN query executes
THEN results are reranked by Jina AI and top 5 chunks have updated scores
2. GIVEN configuration `RERANKER=none`
WHEN query executes
THEN hybrid search results are returned directly without reranking latency
3. GIVEN Jina AI API returns 503 error
WHEN query executes
THEN system logs warning "Reranker unavailable, using hybrid search fallback" and returns Weaviate results

***

## 5. NON-FUNCTIONAL REQUIREMENTS

### 5.1 Performance Requirements

**NFR-P1: Query Response Time**

- **Requirement:** MCP query endpoint SHALL respond within 500ms at p95 for hybrid search without reranking; within 2000ms at p95 with reranking enabled
- **Measurement:** Application Performance Monitoring (APM) via Prometheus histogram metrics; p95 calculated from `http_request_duration_seconds{endpoint="/mcp"}`
- **Test Scenario:** Load test with 100 concurrent users sending 10 queries/second for 5 minutes; measure p50, p95, p99 latencies

**NFR-P2: Ingestion Throughput**

- **Requirement:** System SHALL process at least 1000 documentation pages per hour per CPU core during batch ingestion
- **Measurement:** Track `pages_ingested_total` counter and calculate hourly rate; divide by CPU core count from `runtime.NumCPU()`
- **Test Scenario:** Ingest 10,000-page documentation site; measure total time and calculate pages/hour/core

**NFR-P3: Resource Utilization**

- **CPU:** Maximum 70% average